{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b353729",
   "metadata": {},
   "source": [
    "# UIDAI Hackathon - Data Cleaning & Preprocessing\n",
    "\n",
    "## Objective\n",
    "This notebook performs comprehensive data cleaning and preprocessing on the Aadhaar datasets:\n",
    "- Remove duplicates and invalid records\n",
    "- Normalize dates and text fields\n",
    "- Validate pincodes and geographical data\n",
    "- Engineer temporal features\n",
    "- Create derived metrics\n",
    "- Detect and handle outliers\n",
    "\n",
    "**Author:** Harsh Vardhan  \n",
    "**Date:** January 13, 2026  \n",
    "**Input:** Raw CSV data from Dataset/  \n",
    "**Output:** Cleaned DataFrames ready for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46881c",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0bcf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add src directory to path\n",
    "project_root = Path(r'c:\\Users\\harsh\\OneDrive - Indian Institute of Information Technology, Nagpur\\IIIT Nagpur\\6th Semester\\Projects\\IdentityLab')\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "# Import custom modules\n",
    "from data_loader import AadhaarDataLoader\n",
    "from preprocessing import AadhaarDataPreprocessor, detect_outliers\n",
    "\n",
    "print(\"✓ Environment setup complete\")\n",
    "print(f\"✓ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fa98d",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78993c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = AadhaarDataLoader(str(project_root))\n",
    "\n",
    "print(\"Loading raw datasets...\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enrolment data\n",
    "df_enrolment_raw = loader.load_enrolment_data()\n",
    "print(f\"✓ Loaded {len(df_enrolment_raw):,} raw enrolment records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbfd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographic data\n",
    "df_demographic_raw = loader.load_demographic_data()\n",
    "print(f\"✓ Loaded {len(df_demographic_raw):,} raw demographic records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d19458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load biometric data\n",
    "df_biometric_raw = loader.load_biometric_data()\n",
    "print(f\"✓ Loaded {len(df_biometric_raw):,} raw biometric records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10730615",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning Pipeline\n",
    "\n",
    "Apply systematic cleaning using the AadhaarDataPreprocessor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9038c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = AadhaarDataPreprocessor()\n",
    "print(\"✓ Preprocessor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2e484",
   "metadata": {},
   "source": [
    "### 3.1 Clean Enrolment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean enrolment data\n",
    "df_enrolment = preprocessor.clean_enrolment_data(df_enrolment_raw)\n",
    "\n",
    "print(\"\\nEnrolment Data Sample After Cleaning:\")\n",
    "display(df_enrolment.head())\n",
    "\n",
    "print(\"\\nNew Columns Added:\")\n",
    "print(df_enrolment.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0bc37",
   "metadata": {},
   "source": [
    "### 3.2 Clean Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean demographic data\n",
    "df_demographic = preprocessor.clean_demographic_data(df_demographic_raw)\n",
    "\n",
    "print(\"\\nDemographic Data Sample After Cleaning:\")\n",
    "display(df_demographic.head())\n",
    "\n",
    "print(\"\\nNew Columns Added:\")\n",
    "print(df_demographic.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f7738",
   "metadata": {},
   "source": [
    "### 3.3 Clean Biometric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02481289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean biometric data\n",
    "df_biometric = preprocessor.clean_biometric_data(df_biometric_raw)\n",
    "\n",
    "print(\"\\nBiometric Data Sample After Cleaning:\")\n",
    "display(df_biometric.head())\n",
    "\n",
    "print(\"\\nNew Columns Added:\")\n",
    "print(df_biometric.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff9711",
   "metadata": {},
   "source": [
    "## 4. Cleaning Report\n",
    "\n",
    "Review what was cleaned in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d220dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive cleaning report\n",
    "preprocessor.print_cleaning_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209219ee",
   "metadata": {},
   "source": [
    "## 5. Handle Duplicates\n",
    "\n",
    "Remove duplicate records identified during exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "print(\"Removing duplicate records...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Enrolment\n",
    "enrol_before = len(df_enrolment)\n",
    "df_enrolment = df_enrolment.drop_duplicates()\n",
    "enrol_removed = enrol_before - len(df_enrolment)\n",
    "print(f\"Enrolment: Removed {enrol_removed:,} duplicates ({enrol_removed/enrol_before*100:.2f}%)\")\n",
    "\n",
    "# Demographic\n",
    "demo_before = len(df_demographic)\n",
    "df_demographic = df_demographic.drop_duplicates()\n",
    "demo_removed = demo_before - len(df_demographic)\n",
    "print(f\"Demographic: Removed {demo_removed:,} duplicates ({demo_removed/demo_before*100:.2f}%)\")\n",
    "\n",
    "# Biometric\n",
    "bio_before = len(df_biometric)\n",
    "df_biometric = df_biometric.drop_duplicates()\n",
    "bio_removed = bio_before - len(df_biometric)\n",
    "print(f\"Biometric: Removed {bio_removed:,} duplicates ({bio_removed/bio_before*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e43b1",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection\n",
    "\n",
    "Identify outliers in enrolment counts using IQR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92306b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in enrolment data\n",
    "outliers_enrol = detect_outliers(df_enrolment, 'total_enrolments', method='iqr', threshold=3.0)\n",
    "\n",
    "print(f\"Outliers in Enrolment Data: {outliers_enrol.sum():,} ({outliers_enrol.sum()/len(df_enrolment)*100:.2f}%)\")\n",
    "print(\"\\nOutlier Statistics:\")\n",
    "print(df_enrolment[outliers_enrol]['total_enrolments'].describe())\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample Outlier Records:\")\n",
    "display(df_enrolment[outliers_enrol][['date', 'state', 'district', 'total_enrolments']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in demographic data\n",
    "outliers_demo = detect_outliers(df_demographic, 'total_demo_updates', method='iqr', threshold=3.0)\n",
    "\n",
    "print(f\"Outliers in Demographic Data: {outliers_demo.sum():,} ({outliers_demo.sum()/len(df_demographic)*100:.2f}%)\")\n",
    "print(\"\\nOutlier Statistics:\")\n",
    "print(df_demographic[outliers_demo]['total_demo_updates'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56791bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in biometric data\n",
    "outliers_bio = detect_outliers(df_biometric, 'total_bio_updates', method='iqr', threshold=3.0)\n",
    "\n",
    "print(f\"Outliers in Biometric Data: {outliers_bio.sum():,} ({outliers_bio.sum()/len(df_biometric)*100:.2f}%)\")\n",
    "print(\"\\nOutlier Statistics:\")\n",
    "print(df_biometric[outliers_bio]['total_bio_updates'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce61ec34",
   "metadata": {},
   "source": [
    "## 7. Data Validation\n",
    "\n",
    "Verify cleaned data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate date ranges\n",
    "print(\"Date Range Validation:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Enrolment: {df_enrolment['date'].min()} to {df_enrolment['date'].max()}\")\n",
    "print(f\"Demographic: {df_demographic['date'].min()} to {df_demographic['date'].max()}\")\n",
    "print(f\"Biometric: {df_biometric['date'].min()} to {df_biometric['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate geographical coverage\n",
    "print(\"\\nGeographical Coverage:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Enrolment - States: {df_enrolment['state'].nunique()}, Districts: {df_enrolment['district'].nunique()}\")\n",
    "print(f\"Demographic - States: {df_demographic['state'].nunique()}, Districts: {df_demographic['district'].nunique()}\")\n",
    "print(f\"Biometric - States: {df_biometric['state'].nunique()}, Districts: {df_biometric['district'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate pincode format\n",
    "print(\"\\nPincode Validation:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check if all pincodes are 6 digits\n",
    "invalid_pincodes_enrol = df_enrolment[df_enrolment['pincode'].str.len() != 6]\n",
    "invalid_pincodes_demo = df_demographic[df_demographic['pincode'].str.len() != 6]\n",
    "invalid_pincodes_bio = df_biometric[df_biometric['pincode'].str.len() != 6]\n",
    "\n",
    "print(f\"Invalid pincodes in Enrolment: {len(invalid_pincodes_enrol)}\")\n",
    "print(f\"Invalid pincodes in Demographic: {len(invalid_pincodes_demo)}\")\n",
    "print(f\"Invalid pincodes in Biometric: {len(invalid_pincodes_bio)}\")\n",
    "print(\"\\n✓ All pincodes are properly formatted (6 digits)\" if len(invalid_pincodes_enrol) == 0 else \"⚠ Some pincodes need fixing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b60acf",
   "metadata": {},
   "source": [
    "## 8. Final Summary\n",
    "\n",
    "Summary of cleaned datasets ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "datasets = {\n",
    "    'Enrolment': (df_enrolment_raw, df_enrolment),\n",
    "    'Demographic': (df_demographic_raw, df_demographic),\n",
    "    'Biometric': (df_biometric_raw, df_biometric)\n",
    "}\n",
    "\n",
    "for name, (raw_df, clean_df) in datasets.items():\n",
    "    summary_data.append({\n",
    "        'Dataset': name,\n",
    "        'Raw Records': f\"{len(raw_df):,}\",\n",
    "        'Clean Records': f\"{len(clean_df):,}\",\n",
    "        'Removed': f\"{len(raw_df) - len(clean_df):,}\",\n",
    "        'Removal %': f\"{(len(raw_df) - len(clean_df))/len(raw_df)*100:.2f}%\",\n",
    "        'Final Columns': clean_df.shape[1],\n",
    "        'Memory (MB)': f\"{clean_df.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\n✓ Data cleaning complete!\")\n",
    "print(\"\\nCleaned datasets are ready for:\")\n",
    "print(\"1. Temporal analysis and trend detection\")\n",
    "print(\"2. Geographical pattern analysis\")\n",
    "print(\"3. Cross-dataset correlation studies\")\n",
    "print(\"4. Visualization and reporting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80796d98",
   "metadata": {},
   "source": [
    "## 9. Optional: Save Cleaned Data\n",
    "\n",
    "Save cleaned datasets for reuse (optional step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save cleaned data\n",
    "# output_dir = project_root / 'outputs' / 'cleaned_data'\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# df_enrolment.to_csv(output_dir / 'enrolment_cleaned.csv', index=False)\n",
    "# df_demographic.to_csv(output_dir / 'demographic_cleaned.csv', index=False)\n",
    "# df_biometric.to_csv(output_dir / 'biometric_cleaned.csv', index=False)\n",
    "\n",
    "# print(f\"✓ Cleaned data saved to {output_dir}\")\n",
    "print(\"Cleaned data retained in memory for analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
