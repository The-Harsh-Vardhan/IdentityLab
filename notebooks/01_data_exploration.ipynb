{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753c6c9c",
   "metadata": {},
   "source": [
    "# UIDAI Hackathon - Data Exploration\n",
    "\n",
    "## Objective\n",
    "This notebook performs initial exploration of the Aadhaar enrolment and update datasets to understand:\n",
    "- Data structure and schema\n",
    "- Data quality issues\n",
    "- Basic patterns and trends\n",
    "- Temporal and geographical distributions\n",
    "\n",
    "**Author:** Harsh Vardhan \n",
    "**Date:** January 13, 2026  \n",
    "**Dataset:** Aadhaar Enrolment, Demographic & Biometric Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a9c7e",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Import all necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44df9936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "✓ Project root: c:\\Users\\harsh\\OneDrive - Indian Institute of Information Technology, Nagpur\\IIIT Nagpur\\6th Semester\\Projects\\IdentityLab\n",
      "✓ Pandas version: 2.3.3\n",
      "✓ NumPy version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Add src directory to path\n",
    "project_root = Path(r'c:\\Users\\harsh\\OneDrive - Indian Institute of Information Technology, Nagpur\\IIIT Nagpur\\6th Semester\\Projects\\IdentityLab')\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ Project root: {project_root}\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51df0c4",
   "metadata": {},
   "source": [
    "## 2. Load Datasets\n",
    "\n",
    "Load the three Aadhaar datasets using our custom data loader module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0059650",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import custom data loader\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AadhaarDataLoader\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize the data loader\u001b[39;00m\n\u001b[32m      5\u001b[39m loader = AadhaarDataLoader(\u001b[38;5;28mstr\u001b[39m(project_root))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data_loader'"
     ]
    }
   ],
   "source": [
    "# Import custom data loader\n",
    "from data_loader import AadhaarDataLoader\n",
    "\n",
    "# Initialize the data loader\n",
    "loader = AadhaarDataLoader(str(project_root))\n",
    "\n",
    "print(\"Loading datasets (this may take a few minutes)...\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f537c",
   "metadata": {},
   "source": [
    "### 2.1 Load Enrolment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enrolment data\n",
    "df_enrolment = loader.load_enrolment_data(use_dask=False)\n",
    "print(f\"✓ Enrolment data loaded: {df_enrolment.shape[0]:,} rows × {df_enrolment.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfabdd",
   "metadata": {},
   "source": [
    "### 2.2 Load Demographic Update Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32bb2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographic data\n",
    "df_demographic = loader.load_demographic_data(use_dask=False)\n",
    "print(f\"✓ Demographic data loaded: {df_demographic.shape[0]:,} rows × {df_demographic.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6ec5e",
   "metadata": {},
   "source": [
    "### 2.3 Load Biometric Update Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load biometric data\n",
    "df_biometric = loader.load_biometric_data(use_dask=False)\n",
    "print(f\"✓ Biometric data loaded: {df_biometric.shape[0]:,} rows × {df_biometric.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0bdf9",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration\n",
    "\n",
    "Examine the structure, schema, and first few rows of each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57eba93",
   "metadata": {},
   "source": [
    "### 3.1 Enrolment Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of Enrolment Data:\")\n",
    "display(df_enrolment.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data Info:\")\n",
    "print(\"=\"*80)\n",
    "df_enrolment.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary of Enrolment Data:\")\n",
    "display(df_enrolment.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5143a6",
   "metadata": {},
   "source": [
    "### 3.2 Demographic Update Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0676e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of Demographic Update Data:\")\n",
    "display(df_demographic.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data Info:\")\n",
    "print(\"=\"*80)\n",
    "df_demographic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary of Demographic Update Data:\")\n",
    "display(df_demographic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64093270",
   "metadata": {},
   "source": [
    "### 3.3 Biometric Update Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700daea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of Biometric Update Data:\")\n",
    "display(df_biometric.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data Info:\")\n",
    "print(\"=\"*80)\n",
    "df_biometric.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary of Biometric Update Data:\")\n",
    "display(df_biometric.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98190821",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment\n",
    "\n",
    "Check for missing values, duplicates, data types, and potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8c6f1",
   "metadata": {},
   "source": [
    "### 4.1 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130bd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze missing values\n",
    "def analyze_missing_values(df, name):\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing.index,\n",
    "        'Missing_Count': missing.values,\n",
    "        'Missing_Percentage': missing_pct.values\n",
    "    })\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Missing Values in {name} Dataset\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if len(missing_df) == 0:\n",
    "        print(\"✓ No missing values found!\")\n",
    "    else:\n",
    "        display(missing_df)\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Analyze all datasets\n",
    "missing_enrolment = analyze_missing_values(df_enrolment, \"Enrolment\")\n",
    "missing_demographic = analyze_missing_values(df_demographic, \"Demographic\")\n",
    "missing_biometric = analyze_missing_values(df_biometric, \"Biometric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6b882",
   "metadata": {},
   "source": [
    "### 4.2 Duplicate Records Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebf86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"Duplicate Records:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Enrolment duplicates: {df_enrolment.duplicated().sum():,}\")\n",
    "print(f\"Demographic duplicates: {df_demographic.duplicated().sum():,}\")\n",
    "print(f\"Biometric duplicates: {df_biometric.duplicated().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf7a7f",
   "metadata": {},
   "source": [
    "### 4.3 Unique Values Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea79b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values for categorical columns\n",
    "print(\"Unique Values in Enrolment Data:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Unique States: {df_enrolment['state'].nunique()}\")\n",
    "print(f\"Unique Districts: {df_enrolment['district'].nunique()}\")\n",
    "print(f\"Unique Pincodes: {df_enrolment['pincode'].nunique()}\")\n",
    "print(f\"Date Range: {df_enrolment['date'].min()} to {df_enrolment['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233948fb",
   "metadata": {},
   "source": [
    "## 5. Basic Visualizations\n",
    "\n",
    "Create initial visualizations to understand the data distributions and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f5b9d",
   "metadata": {},
   "source": [
    "### 5.1 Sample Records from Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d32dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random samples\n",
    "print(\"Random samples from each dataset:\\n\")\n",
    "print(\"Enrolment Sample:\")\n",
    "display(df_enrolment.sample(3))\n",
    "\n",
    "print(\"\\nDemographic Sample:\")\n",
    "display(df_demographic.sample(3))\n",
    "\n",
    "print(\"\\nBiometric Sample:\")\n",
    "display(df_biometric.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6266ee",
   "metadata": {},
   "source": [
    "## 6. Summary and Initial Insights\n",
    "\n",
    "Summarize key findings from this initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "datasets = {\n",
    "    'Enrolment': df_enrolment,\n",
    "    'Demographic': df_demographic,\n",
    "    'Biometric': df_biometric\n",
    "}\n",
    "\n",
    "summary_data = []\n",
    "for name, df in datasets.items():\n",
    "    summary_data.append({\n",
    "        'Dataset': name,\n",
    "        'Total Records': f\"{len(df):,}\",\n",
    "        'Columns': df.shape[1],\n",
    "        'Memory (MB)': f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f}\",\n",
    "        'Missing Values': df.isnull().sum().sum(),\n",
    "        'Duplicates': df.duplicated().sum()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\n✓ Data exploration complete!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Data cleaning and preprocessing\")\n",
    "print(\"2. Detailed temporal analysis\")\n",
    "print(\"3. Geographical pattern analysis\")\n",
    "print(\"4. Cross-dataset correlation analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
